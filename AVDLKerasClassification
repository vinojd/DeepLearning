Skip to main content
Loan_Prediction_Data_Preparation.ipynb
Loan_Prediction_Data_Preparation.ipynb_Notebook unstarred
Last saved at 11:08â€¯PM
Pre-processing Steps
1. Filling the missing values
2. Converting categories to numbers
3. Bring all the variables in range 0 to 1


# importing required libraries
import pandas as pd


# check version on pandas
print('Version of pandas:', pd.__version__)
Version of pandas: 0.25.3


# reading the loan prediction data
data = pd.read_csv('loan_data.csv')


# looking at the first five rows of the data
data.head()



# shape of the data
data.shape
(614, 13)


# checking missing values in the data
data.isnull().sum()
Loan_ID               0
Gender               13
Married               3
Dependents           15
Education             0
Self_Employed        32
ApplicantIncome       0
CoapplicantIncome     0
LoanAmount           22
Loan_Amount_Term     14
Credit_History       50
Property_Area         0
Loan_Status           0
dtype: int64


# data types of the variables
data.dtypes
Loan_ID               object
Gender                object
Married               object
Dependents            object
Education             object
Self_Employed         object
ApplicantIncome        int64
CoapplicantIncome    float64
LoanAmount           float64
Loan_Amount_Term     float64
Credit_History       float64
Property_Area         object
Loan_Status           object
dtype: object
1. Filling the missing values
Categorical Data: Mode


# filling missing values of categorical variables with mode

data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)

data['Married'].fillna(data['Married'].mode()[0], inplace=True)

data['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)

data['Self_Employed'].fillna(data['Self_Employed'].mode()[0], inplace=True)

data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)

data['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)
Continuous Data: Mean


# filling missing values of continuous variables with mean
data['LoanAmount'].fillna(data['LoanAmount'].mean(), inplace=True)


# checking missing values after imputation
data.isnull().sum()
Loan_ID               0
Gender               13
Married               3
Dependents           15
Education             0
Self_Employed        32
ApplicantIncome       0
CoapplicantIncome     0
LoanAmount            0
Loan_Amount_Term     14
Credit_History       50
Property_Area         0
Loan_Status           0
dtype: int64
2. Converting categories to numbers


# converting the categories into numbers using map function
data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})
data['Married'] = data['Married'].map({'No': 0, 'Yes': 1})
data['Dependents'] = data['Dependents'].map({'0': 0, '1': 1, '2': 2, '3+': 3})
data['Education'] = data['Education'].map({'Graduate': 1, 'Not Graduate': 0})
data['Self_Employed'] = data['Self_Employed'].map({'No': 0, 'Yes': 1})
data['Property_Area'] = data['Property_Area'].map({'Rural': 0, 'Semiurban': 1, 'Urban': 2})
data['Loan_Status'] = data['Loan_Status'].map({'N': 0, 'Y': 1})


data.head()

3. Bringing all the variables in range 0 to 1




# bringing variables in the range 0 to 1
data['Dependents']=(data['Dependents']-data['Dependents'].min())/(data['Dependents'].max()-data['Dependents'].min())


# applying for loop to bring all the variables in range 0 to 1

for i in data.columns[1:]:
    data[i] = (data[i] - data[i].min()) / (data[i].max() - data[i].min())


# again looking at first five rows of pre-processed data
data.head()



# saving the pre-processed data
data.to_csv('loan_prediction_data.csv', index=False)





Steps to build a Neural Network using Keras
1. Loading the dataset
2. Creating training and validation set
3. Defining the architecture of the model
4. Compiling the model (defining loss function, optimizer)
5. Training the model
6. Evaluating model performance on training and validation set


1. Loading the dataset
# importing the required libraries
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
%matplotlib inline


# check version on sklearn
print('Version of sklearn:', sklearn.__version__)

# loading the pre-processed dataset
data = pd.read_csv('loan_prediction_data.csv')

# looking at the first five rows of the dataset
data.head()

# checking missing values
data.isnull().sum()

# checking the data type
data.dtypes

# looking at the shape of the data
data.shape

# separating the independent and dependent variables

# storing all the independent variables as X
X = data.drop('Loan_Status', axis=1)

# storing the dependent variable as y
y = data['Loan_Status']

# shape of independent and dependent variables
X.shape, y.shape


2. Creating training and validation set
# Creating training and validation set

# stratify will make sure that the distribution of classes in train and validation set it similar
# random state to regenerate the same train and validation set
# test size 0.2 will keep 20% data in validation and remaining 80% in train set

X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=data['Loan_Status'],random_state=10,test_size=0.2)

# shape of training and validation set
(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)


3. Defining the architecture of the model

# checking the version of keras
import keras
print(keras.__version__)
Using TensorFlow backend.
2.2.5

# checking the version of tensorflow
import tensorflow as tf
print(tf.__version__)
1.15.2

Create a model
# importing the sequential model
from keras.models import Sequential

# importing different layers from keras
from keras.layers import InputLayer, Dense 

# number of input neurons
X_train.shape

# number of features in the data
X_train.shape[1]

# number of features in the data
X_train.shape[1]

# defining input neurons
input_neurons = X_train.shape[1]

# number of output neurons
# since loan prediction is a binary classification problem, we will have single neuron in the output layer 
# define number of output neurons
output_neurons = 1

# number of hidden layers and hidden neurons
# It is a hyperparameter and we can pick the hidden layers and hidden neurons on our own

# define hidden layers and neuron in each layer
number_of_hidden_layers = 2
neuron_hidden_layer_1 = 10
neuron_hidden_layer_2 = 5

# activation function of different layers
# for now I have picked relu as an activation function for hidden layers, you can change it as well
# since it is a binary classification problem, I have used sigmoid activation function in the final layer

# defining the architecture of the model
model = Sequential()
model.add(InputLayer(input_shape=(input_neurons,)))
model.add(Dense(units=neuron_hidden_layer_1, activation='relu'))
model.add(Dense(units=neuron_hidden_layer_2, activation='relu'))
model.add(Dense(units=output_neurons, activation='sigmoid'))

# summary of the model
model.summary()

# number of parameters between input and first hidden layer
input_neurons*neuron_hidden_layer_1

# number of parameters between input and first hidden layer
# adding the bias for each neuron of first hidden layer
input_neurons*neuron_hidden_layer_1 + 10

# number of parameters between first and second hidden layer
neuron_hidden_layer_1*neuron_hidden_layer_2 + 5

# number of parameters between second hidden and output layer
neuron_hidden_layer_2*output_neurons + 1

4. Compiling the model (defining loss function, optimizer)
# compiling the model

# loss as binary_crossentropy, since we have binary classification problem
# defining the optimizer as adam
# Evaluation metric as accuracy
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])

5. Training the model

# training the model
# passing the independent and dependent features for training set for training the model
# validation data will be evaluated at the end of each epoch
# setting the epochs as 50
# storing the trained model in model_history variable which will be used to visualize the training process
model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)

6. Evaluating model performance on validation set
# getting predictions for the validation set
prediction = np.where(model.predict(X_test) < 0.5, 0 , 1)

# calculating the accuracy on validation set
accuracy_score(y_test, prediction)

Visualizing the model performance
# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

